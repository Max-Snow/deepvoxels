{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load('/home/max/saved_data/train_repre/tr_labels.npy')\n",
    "test_labels = np.load('/home/max/saved_data/test_repre/te_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, train=True):\n",
    "        super().__init__()\n",
    "        self.train = train\n",
    "        if train == True:\n",
    "            self.all_repre = sorted(glob(os.path.join(root_dir, 'train_*.npy')))\n",
    "        if train == False:\n",
    "            self.all_repre = sorted(glob(os.path.join(root_dir, 'test_*.npy')))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_repre)        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        repre = np.load(self.all_repre[idx]).squeeze()\n",
    "        if self.train == True:\n",
    "            return (repre, train_labels[idx])\n",
    "        if self.train == False:\n",
    "            return (repre, test_labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(os.path.join('/home/max/saved_data/train_repre'), True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "test_dataset = Dataset(os.path.join('/home/max/saved_data/test_repre'), False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=1):\n",
    "\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x,y) in enumerate(train_loader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % 20 == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                \n",
    "        torch.save(model.state_dict(), os.path.join('/home/max/saved_data/classifier/model_epoch'+str(e)+'.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.conv1 = nn.Conv3d(64, 64, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv3d(64, 64, kernel_size = 3, padding = 1)\n",
    "        self.conv3 = nn.Conv3d(64, 64, kernel_size = 3, padding = 1)\n",
    "        self.conv4 = nn.Conv3d(64, 64, kernel_size = 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(64*2*2*2, 64)\n",
    "        self.fc2 = nn.Linear(64, 13)\n",
    "        self.maxpool = nn.MaxPool3d(2, stride=2, padding=0)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.conv3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.conv4(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = out.view(out.shape[0],-1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        scores = self.fc2(out)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.5198\n",
      "Iteration 20, loss = 2.6036\n",
      "Iteration 40, loss = 2.6084\n",
      "Iteration 60, loss = 2.1930\n",
      "Iteration 80, loss = 2.5093\n",
      "Iteration 100, loss = 2.4267\n",
      "Iteration 120, loss = 1.6189\n",
      "Iteration 140, loss = 2.1803\n",
      "Iteration 160, loss = 1.5879\n",
      "Iteration 180, loss = 2.4693\n",
      "Iteration 200, loss = 1.7325\n",
      "Iteration 0, loss = 0.6517\n",
      "Iteration 20, loss = 2.7580\n",
      "Iteration 40, loss = 0.2630\n",
      "Iteration 60, loss = 1.1348\n",
      "Iteration 80, loss = 0.8590\n",
      "Iteration 100, loss = 0.2543\n",
      "Iteration 120, loss = 0.4734\n",
      "Iteration 140, loss = 1.3644\n",
      "Iteration 160, loss = 0.3079\n",
      "Iteration 180, loss = 0.7578\n",
      "Iteration 200, loss = 0.5828\n",
      "Iteration 0, loss = 0.5882\n",
      "Iteration 20, loss = 0.4171\n",
      "Iteration 40, loss = 0.3173\n",
      "Iteration 60, loss = 0.8344\n",
      "Iteration 80, loss = 0.6031\n",
      "Iteration 100, loss = 0.5487\n",
      "Iteration 120, loss = 0.3258\n",
      "Iteration 140, loss = 0.4382\n",
      "Iteration 160, loss = 0.7018\n",
      "Iteration 180, loss = 1.5920\n",
      "Iteration 200, loss = 1.3183\n",
      "Iteration 0, loss = 0.8249\n",
      "Iteration 20, loss = 0.3855\n",
      "Iteration 40, loss = 0.8421\n",
      "Iteration 60, loss = 0.6234\n",
      "Iteration 80, loss = 0.5220\n",
      "Iteration 100, loss = 0.8919\n",
      "Iteration 120, loss = 1.1113\n",
      "Iteration 140, loss = 0.8967\n",
      "Iteration 160, loss = 0.3552\n",
      "Iteration 180, loss = 0.0637\n",
      "Iteration 200, loss = 1.0800\n",
      "Iteration 0, loss = 1.1126\n",
      "Iteration 20, loss = 0.0780\n",
      "Iteration 40, loss = 0.0107\n",
      "Iteration 60, loss = 0.3981\n",
      "Iteration 80, loss = 0.0072\n",
      "Iteration 100, loss = 0.0124\n",
      "Iteration 120, loss = 0.2058\n",
      "Iteration 140, loss = 0.2107\n",
      "Iteration 160, loss = 0.2733\n",
      "Iteration 180, loss = 0.2832\n",
      "Iteration 200, loss = 0.2475\n",
      "Iteration 0, loss = 1.0530\n",
      "Iteration 20, loss = 0.0105\n",
      "Iteration 40, loss = 0.1351\n",
      "Iteration 60, loss = 0.0180\n",
      "Iteration 80, loss = 0.0856\n",
      "Iteration 100, loss = 1.1638\n",
      "Iteration 120, loss = 1.3341\n",
      "Iteration 140, loss = 0.1346\n",
      "Iteration 160, loss = 0.3155\n",
      "Iteration 180, loss = 0.3170\n",
      "Iteration 200, loss = 0.3806\n",
      "Iteration 0, loss = 0.1864\n",
      "Iteration 20, loss = 0.4421\n",
      "Iteration 40, loss = 0.0155\n",
      "Iteration 60, loss = 0.4698\n",
      "Iteration 80, loss = 0.4108\n",
      "Iteration 100, loss = 0.0071\n",
      "Iteration 120, loss = 0.0011\n",
      "Iteration 140, loss = 0.0869\n",
      "Iteration 160, loss = 0.1778\n",
      "Iteration 180, loss = 0.0176\n",
      "Iteration 200, loss = 0.5692\n",
      "Iteration 0, loss = 0.0200\n",
      "Iteration 20, loss = 0.3435\n",
      "Iteration 40, loss = 0.0045\n",
      "Iteration 60, loss = 0.0490\n",
      "Iteration 80, loss = 0.1466\n",
      "Iteration 100, loss = 0.0286\n",
      "Iteration 120, loss = 0.0020\n",
      "Iteration 140, loss = 0.1208\n",
      "Iteration 160, loss = 0.0045\n",
      "Iteration 180, loss = 0.0000\n",
      "Iteration 200, loss = 0.0070\n",
      "Iteration 0, loss = 0.0033\n",
      "Iteration 20, loss = 0.1209\n",
      "Iteration 40, loss = 0.0220\n",
      "Iteration 60, loss = 0.0250\n",
      "Iteration 80, loss = 0.0225\n",
      "Iteration 100, loss = 0.7423\n",
      "Iteration 120, loss = 0.0689\n",
      "Iteration 140, loss = 0.1230\n",
      "Iteration 160, loss = 0.0236\n",
      "Iteration 180, loss = 0.0147\n",
      "Iteration 200, loss = 0.0141\n",
      "Iteration 0, loss = 0.1633\n",
      "Iteration 20, loss = 0.4644\n",
      "Iteration 40, loss = 0.0025\n",
      "Iteration 60, loss = 0.0031\n",
      "Iteration 80, loss = 0.0001\n",
      "Iteration 100, loss = 0.0081\n",
      "Iteration 120, loss = 0.0002\n",
      "Iteration 140, loss = 0.0536\n",
      "Iteration 160, loss = 0.0001\n",
      "Iteration 180, loss = 0.0066\n",
      "Iteration 200, loss = 0.0080\n",
      "Iteration 0, loss = 0.0040\n",
      "Iteration 20, loss = 0.0038\n",
      "Iteration 40, loss = 0.0002\n",
      "Iteration 60, loss = 0.0037\n",
      "Iteration 80, loss = 0.0001\n",
      "Iteration 100, loss = 0.6798\n",
      "Iteration 120, loss = 0.0126\n",
      "Iteration 140, loss = 0.0883\n",
      "Iteration 160, loss = 0.0090\n",
      "Iteration 180, loss = 0.0612\n",
      "Iteration 200, loss = 0.4818\n",
      "Iteration 0, loss = 0.0052\n",
      "Iteration 20, loss = 0.3260\n",
      "Iteration 40, loss = 0.8175\n",
      "Iteration 60, loss = 0.0230\n",
      "Iteration 80, loss = 0.0208\n",
      "Iteration 100, loss = 0.0002\n",
      "Iteration 120, loss = 0.0004\n",
      "Iteration 140, loss = 0.0648\n",
      "Iteration 160, loss = 0.6224\n",
      "Iteration 180, loss = 0.4351\n",
      "Iteration 200, loss = 0.4135\n",
      "Iteration 0, loss = 0.0070\n",
      "Iteration 20, loss = 0.1554\n",
      "Iteration 40, loss = 0.0162\n",
      "Iteration 60, loss = 0.0102\n",
      "Iteration 80, loss = 0.0003\n",
      "Iteration 100, loss = 0.0008\n",
      "Iteration 120, loss = 0.1499\n",
      "Iteration 140, loss = 0.0606\n",
      "Iteration 160, loss = 0.0229\n",
      "Iteration 180, loss = 0.1517\n",
      "Iteration 200, loss = 0.1917\n",
      "Iteration 0, loss = 0.0000\n",
      "Iteration 20, loss = 0.0002\n",
      "Iteration 40, loss = 0.0022\n",
      "Iteration 60, loss = 0.0001\n",
      "Iteration 80, loss = 0.0002\n",
      "Iteration 100, loss = 0.0219\n",
      "Iteration 120, loss = 0.0062\n",
      "Iteration 140, loss = 0.0076\n",
      "Iteration 160, loss = 0.5470\n",
      "Iteration 180, loss = 0.1175\n",
      "Iteration 200, loss = 0.0026\n",
      "Iteration 0, loss = 0.0030\n",
      "Iteration 20, loss = 0.0046\n",
      "Iteration 40, loss = 0.0000\n",
      "Iteration 60, loss = 0.1422\n",
      "Iteration 80, loss = 0.0009\n",
      "Iteration 100, loss = 0.1817\n",
      "Iteration 120, loss = 0.0028\n",
      "Iteration 140, loss = 0.8855\n",
      "Iteration 160, loss = 0.0399\n",
      "Iteration 180, loss = 0.0034\n",
      "Iteration 200, loss = 0.0000\n",
      "Iteration 0, loss = 0.0420\n",
      "Iteration 20, loss = 0.0113\n",
      "Iteration 40, loss = 0.0000\n",
      "Iteration 60, loss = 0.0320\n",
      "Iteration 80, loss = 0.0006\n",
      "Iteration 100, loss = 0.0022\n",
      "Iteration 120, loss = 0.0003\n",
      "Iteration 140, loss = 0.0010\n",
      "Iteration 160, loss = 0.0203\n",
      "Iteration 180, loss = 0.0024\n",
      "Iteration 200, loss = 0.0002\n",
      "Iteration 0, loss = 0.0028\n",
      "Iteration 20, loss = 0.0018\n",
      "Iteration 40, loss = 0.0004\n",
      "Iteration 60, loss = 0.0006\n",
      "Iteration 80, loss = 0.0005\n",
      "Iteration 100, loss = 0.0003\n",
      "Iteration 120, loss = 0.0000\n",
      "Iteration 140, loss = 0.0102\n",
      "Iteration 160, loss = 0.0000\n",
      "Iteration 180, loss = 0.0301\n",
      "Iteration 200, loss = 0.0000\n",
      "Iteration 0, loss = 0.0036\n",
      "Iteration 20, loss = 0.0000\n",
      "Iteration 40, loss = 0.0006\n",
      "Iteration 60, loss = 0.0000\n",
      "Iteration 80, loss = 0.0001\n",
      "Iteration 100, loss = 0.0072\n",
      "Iteration 120, loss = 0.0000\n",
      "Iteration 140, loss = 0.0000\n",
      "Iteration 160, loss = 0.0001\n",
      "Iteration 180, loss = 0.0056\n",
      "Iteration 200, loss = 0.2673\n",
      "Iteration 0, loss = 0.0150\n",
      "Iteration 20, loss = 0.0130\n",
      "Iteration 40, loss = 0.0000\n",
      "Iteration 60, loss = 0.0000\n",
      "Iteration 80, loss = 0.0392\n",
      "Iteration 100, loss = 0.0077\n",
      "Iteration 120, loss = 0.2694\n",
      "Iteration 140, loss = 0.0051\n",
      "Iteration 160, loss = 0.0001\n",
      "Iteration 180, loss = 0.0039\n",
      "Iteration 200, loss = 0.0017\n",
      "Iteration 0, loss = 0.0010\n",
      "Iteration 20, loss = 0.0000\n",
      "Iteration 40, loss = 0.0233\n",
      "Iteration 60, loss = 0.1064\n",
      "Iteration 80, loss = 1.2478\n",
      "Iteration 100, loss = 0.0033\n",
      "Iteration 120, loss = 0.3791\n",
      "Iteration 140, loss = 0.0236\n",
      "Iteration 160, loss = 0.0160\n",
      "Iteration 180, loss = 0.0278\n",
      "Iteration 200, loss = 0.0062\n",
      "Iteration 0, loss = 0.1059\n",
      "Iteration 20, loss = 0.0076\n",
      "Iteration 40, loss = 0.0005\n",
      "Iteration 60, loss = 0.0008\n",
      "Iteration 80, loss = 0.0000\n",
      "Iteration 100, loss = 0.0000\n",
      "Iteration 120, loss = 0.1408\n",
      "Iteration 140, loss = 0.3337\n",
      "Iteration 160, loss = 0.0001\n",
      "Iteration 180, loss = 0.0000\n",
      "Iteration 200, loss = 0.0002\n",
      "Iteration 0, loss = 0.0007\n",
      "Iteration 20, loss = 0.0000\n",
      "Iteration 40, loss = 0.0046\n",
      "Iteration 60, loss = 0.0000\n",
      "Iteration 80, loss = 0.0329\n",
      "Iteration 100, loss = 0.0198\n",
      "Iteration 120, loss = 0.0000\n",
      "Iteration 140, loss = 0.0087\n",
      "Iteration 160, loss = 0.0004\n",
      "Iteration 180, loss = 0.0001\n",
      "Iteration 200, loss = 0.0005\n",
      "Iteration 0, loss = 0.0006\n",
      "Iteration 20, loss = 0.0000\n",
      "Iteration 40, loss = 0.0000\n",
      "Iteration 60, loss = 0.0141\n",
      "Iteration 80, loss = 0.1636\n",
      "Iteration 100, loss = 0.0000\n",
      "Iteration 120, loss = 0.0026\n",
      "Iteration 140, loss = 0.0000\n",
      "Iteration 160, loss = 0.0000\n",
      "Iteration 180, loss = 0.0003\n",
      "Iteration 200, loss = 0.0005\n",
      "Iteration 0, loss = 0.8210\n",
      "Iteration 20, loss = 0.0085\n",
      "Iteration 40, loss = 0.0003\n",
      "Iteration 60, loss = 0.0005\n",
      "Iteration 80, loss = 0.0015\n",
      "Iteration 100, loss = 0.0011\n",
      "Iteration 120, loss = 0.0007\n",
      "Iteration 140, loss = 0.0000\n",
      "Iteration 160, loss = 0.0004\n",
      "Iteration 180, loss = 0.0008\n",
      "Iteration 200, loss = 0.0003\n",
      "Iteration 0, loss = 0.0012\n",
      "Iteration 20, loss = 0.3580\n",
      "Iteration 40, loss = 0.0000\n",
      "Iteration 60, loss = 0.0003\n",
      "Iteration 80, loss = 0.0000\n",
      "Iteration 100, loss = 0.0000\n",
      "Iteration 120, loss = 0.0015\n",
      "Iteration 140, loss = 0.0001\n",
      "Iteration 160, loss = 0.0000\n",
      "Iteration 180, loss = 0.0000\n",
      "Iteration 200, loss = 0.0013\n",
      "Iteration 0, loss = 0.0000\n",
      "Iteration 20, loss = 0.0000\n",
      "Iteration 40, loss = 0.0000\n",
      "Iteration 60, loss = 0.0000\n",
      "Iteration 80, loss = 0.0000\n",
      "Iteration 100, loss = 0.0414\n",
      "Iteration 120, loss = 0.0000\n",
      "Iteration 140, loss = 0.0000\n",
      "Iteration 160, loss = 0.0000\n",
      "Iteration 180, loss = 0.0012\n",
      "Iteration 200, loss = 0.0000\n",
      "Iteration 0, loss = 0.0000\n",
      "Iteration 20, loss = 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 0.0077\n",
      "Iteration 60, loss = 0.0019\n",
      "Iteration 80, loss = 0.0000\n",
      "Iteration 100, loss = 0.0151\n",
      "Iteration 120, loss = 0.0000\n",
      "Iteration 140, loss = 0.0125\n",
      "Iteration 160, loss = 0.0000\n",
      "Iteration 180, loss = 0.0030\n",
      "Iteration 200, loss = 0.1791\n",
      "Iteration 0, loss = 0.0009\n",
      "Iteration 20, loss = 0.0000\n",
      "Iteration 40, loss = 0.0170\n",
      "Iteration 60, loss = 0.0033\n",
      "Iteration 80, loss = 0.0000\n",
      "Iteration 100, loss = 0.0001\n",
      "Iteration 120, loss = 0.0139\n",
      "Iteration 140, loss = 0.0000\n",
      "Iteration 160, loss = 0.0095\n",
      "Iteration 180, loss = 2.0894\n",
      "Iteration 200, loss = 0.0022\n",
      "Iteration 0, loss = 0.0035\n",
      "Iteration 20, loss = 0.0038\n",
      "Iteration 40, loss = 0.0004\n",
      "Iteration 60, loss = 0.0004\n",
      "Iteration 80, loss = 0.0000\n",
      "Iteration 100, loss = 0.0000\n",
      "Iteration 120, loss = 0.0000\n",
      "Iteration 140, loss = 0.0000\n",
      "Iteration 160, loss = 0.0003\n",
      "Iteration 180, loss = 0.0000\n",
      "Iteration 200, loss = 0.0000\n",
      "Iteration 0, loss = 0.0000\n",
      "Iteration 20, loss = 0.0000\n",
      "Iteration 40, loss = 0.0000\n",
      "Iteration 60, loss = 0.0000\n",
      "Iteration 80, loss = 0.0000\n",
      "Iteration 100, loss = 0.0000\n",
      "Iteration 120, loss = 0.0000\n",
      "Iteration 140, loss = 0.0002\n",
      "Iteration 160, loss = 0.0205\n",
      "Iteration 180, loss = 0.0124\n",
      "Iteration 200, loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train(model, optimizer, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part(loader, model): \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model = model.to(device=device)\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 160 / 260 correct (61.54)\n",
      "Got 184 / 260 correct (70.77)\n",
      "Got 201 / 260 correct (77.31)\n",
      "Got 199 / 260 correct (76.54)\n",
      "Got 195 / 260 correct (75.00)\n",
      "Got 199 / 260 correct (76.54)\n",
      "Got 203 / 260 correct (78.08)\n",
      "Got 201 / 260 correct (77.31)\n",
      "Got 200 / 260 correct (76.92)\n",
      "Got 200 / 260 correct (76.92)\n",
      "Got 205 / 260 correct (78.85)\n",
      "Got 195 / 260 correct (75.00)\n",
      "Got 203 / 260 correct (78.08)\n",
      "Got 204 / 260 correct (78.46)\n",
      "Got 207 / 260 correct (79.62)\n"
     ]
    }
   ],
   "source": [
    "accs = list()\n",
    "for i in range(0,30,2):\n",
    "    model = ConvNet()\n",
    "    model.load_state_dict(torch.load(os.path.join('/home/max/saved_data/classifier/model_epoch'+str(i)+'.pth')))\n",
    "    acc = check_accuracy_part(test_loader, model)\n",
    "    accs.append(acc)\n",
    "\n",
    "accs = np.array(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(),accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
